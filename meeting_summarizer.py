# -*- coding: utf-8 -*-
"""Meeting Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DnYj8ndZBIJjhErwNuD-dP7hhqyZnWtO
"""



import asyncio

async def transcribe_file(file_path: str, provider: str = "openai"):
    """Mock ASR (speech-to-text) function."""
    await asyncio.sleep(1)  # simulate processing time
    return f"Transcription of {file_path} using {provider} provider."

import asyncio

async def summarize_transcript(transcript: str, metadata: dict = None):
    """Mock summarization function."""
    await asyncio.sleep(1)  # simulate processing time
    summary = "This is a short summary of the provided transcript."
    return {
        "summary": summary,
        "word_count": len(transcript.split()),
        "metadata": metadata or {}
    }

import os
from fastapi import FastAPI, UploadFile, File, HTTPException
import aiofiles

# Directory for uploaded audio files
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

app = FastAPI(title="Meeting Summarizer API")

@app.post("/upload")
async def upload_audio(file: UploadFile = File(...), asr_provider: str = "openai"):
    """Endpoint to upload an audio file, transcribe it, and summarize the transcript."""

    # Save uploaded file
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    try:
        async with aiofiles.open(file_path, "wb") as out_file:
            content = await file.read()
            await out_file.write(content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"File saving failed: {str(e)}")

    # Transcribe audio
    try:
        transcript = await transcribe_file(file_path, provider=asr_provider)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")

    # Summarize transcript
    try:
        summary_obj = await summarize_transcript(transcript, metadata={"filename": file.filename})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Summarization failed: {str(e)}")

    return {
        "filename": file.filename,
        "transcript": transcript,
        "summary": summary_obj
    }

# backend/asr.py
import os
import asyncio
import openai
import requests

openai.api_key = os.getenv("OPENAI_API_KEY")

async def transcribe_with_openai(file_path):
    # synchronous call wrapped for simplicity
    with open(file_path, "rb") as f:
        # If using the 'transcriptions' endpoint (varies by OpenAI SDK version)
        # This is a representative example — check the exact SDK method in docs.
        resp = openai.Audio.transcriptions.create(model="gpt-4o-mini-transcribe", file=f)
        # OR if using the REST endpoint: use requests.post to /v1/audio/transcriptions
    return resp["text"]  # adjust depending on SDK response

# Azure REST example
import requests, time
AZURE_REGION = os.getenv("AZURE_REGION")
AZURE_KEY = os.getenv("AZURE_KEY")

def transcribe_with_azure(file_path):
    # For short audio, Azure supports a quick REST POST (see docs). This is a minimal example.
    token_url = f"https://{AZURE_REGION}.api.cognitive.microsoft.com/sts/v1.0/issueToken"
    headers = {"Ocp-Apim-Subscription-Key": AZURE_KEY}
    token = requests.post(token_url, headers=headers).text
    stt_url = f"https://{AZURE_REGION}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1"
    with open(file_path, "rb") as f:
        audio_bytes = f.read()
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "audio/wav"
    }
    r = requests.post(stt_url, headers=headers, data=audio_bytes, params={"language": "en-US"})
    r.raise_for_status()
    return r.json().get("DisplayText", "")

# Top-level adapter
async def transcribe_file(file_path, provider="openai"):
    if provider == "openai":
        return await asyncio.to_thread(transcribe_with_openai, file_path)
    elif provider == "azure":
        return await asyncio.to_thread(transcribe_with_azure, file_path)
    else:
        raise ValueError("provider not supported")

# backend/summarizer.py
import os
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

SYSTEM_PROMPT = """You are a meeting summarization assistant. Given a transcript produce a JSON object with keys:
short_summary, decisions, action_items, participants, important_topics.
Action items should include owner (or null) and due (or null). Return only JSON.
"""

def build_prompt(transcript, filename=None):
    return SYSTEM_PROMPT + "\n\nTranscript:\n" + transcript

async def summarize_transcript(transcript, metadata=None):
    prompt = build_prompt(transcript, metadata.get("filename") if metadata else None)
    # call ChatCompletion (example)
    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",  # pick an appropriate model you have access to
        messages=[
            {"role":"system", "content": SYSTEM_PROMPT},
            {"role":"user", "content": prompt}
        ],
        temperature=0.0,
        max_tokens=800
    )
    out = resp["choices"][0]["message"]["content"]
    # try to parse JSON
    import json
    try:
        parsed = json.loads(out)
    except Exception:
        # fallback: return raw text
        parsed = {"raw": out}
    return parsed

"""
FastAPI backend for Meeting Summarizer.
Endpoints:
 - POST /upload: upload audio -> transcribe -> summarize -> return transcript+summary
"""

import os
import asyncio
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
import aiofiles
from dotenv import load_dotenv

load_dotenv()

UPLOAD_DIR = os.getenv("UPLOAD_DIR", "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

app = FastAPI(title="Meeting Summarizer API")

# allow local frontend during dev
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"status": "ok", "message": "Meeting Summarizer API"}

@app.post("/upload")
async def upload_audio(
    file: UploadFile = File(...),
    asr_provider: str = Form("openai"),
    meeting_title: str = Form(None)
):
    # Save uploaded file
    filename = file.filename or "upload_audio"
    file_path = os.path.join(UPLOAD_DIR, filename)
    try:
        async with aiofiles.open(file_path, "wb") as out:
            content = await file.read()
            await out.write(content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save file: {e}")

    # Run ASR adapter
    try:
        transcript = await transcribe_file(file_path, provider=asr_provider)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ASR error: {e}")

    # Summarize with LLM
    try:
        summary_obj = await summarize_transcript(transcript, metadata={"filename": filename, "meeting_title": meeting_title})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Summarizer error: {e}")

    return {"transcript": transcript, "summary": summary_obj, "filename": filename}

"""
ASR adapters.
- transcribe_file(file_path, provider="openai")
Supports 'openai' and 'azure' out of the box.
(You can add 'google' adapter easily.)
"""

import os
import asyncio
import json
import requests
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
AZURE_KEY = os.getenv("AZURE_KEY")
AZURE_REGION = os.getenv("AZURE_REGION")

# Prefer to import openai only when used, to avoid hard dependency issues in some envs
def transcribe_with_openai(file_path):
    """
    Uses OpenAI's audio transcription endpoint.
    This example uses a generic HTTP approach (works if your SDK version differs).
    Check OpenAI docs for latest endpoint and model name.
    """
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY is not set in env")
    url = "https://api.openai.com/v1/audio/transcriptions"
    # model choice may change with OpenAI versions; use 'gpt-4o-mini-transcribe' or 'whisper-1' per your access
    model = "whisper-1"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}
    with open(file_path, "rb") as f:
        files = {
            "file": (os.path.basename(file_path), f),
        }
        data = {"model": model}
        r = requests.post(url, headers=headers, files=files, data=data, timeout=300)
    r.raise_for_status()
    resp = r.json()
    # typical response: {"text": "..."}
    text = resp.get("text") or resp.get("transcript") or json.dumps(resp)
    return text

def transcribe_with_azure(file_path):
    """
    Minimal Azure Speech REST example for small files.
    For production, use the official Azure SDK and batch transcription for long files.
    """
    if not AZURE_KEY or not AZURE_REGION:
        raise RuntimeError("AZURE_KEY or AZURE_REGION not set")
    # Get token
    token_url = f"https://{AZURE_REGION}.api.cognitive.microsoft.com/sts/v1.0/issueToken"
    token_resp = requests.post(token_url, headers={"Ocp-Apim-Subscription-Key": AZURE_KEY}, timeout=20)
    token_resp.raise_for_status()
    token = token_resp.text
    # Recognize (short audio)
    stt_url = f"https://{AZURE_REGION}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1"
    params = {"language": "en-US"}
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "audio/wav"
    }
    with open(file_path, "rb") as f:
        audio_bytes = f.read()
    r = requests.post(stt_url, headers=headers, params=params, data=audio_bytes, timeout=300)
    r.raise_for_status()
    resp = r.json()
    # Azure returns e.g. {"RecognitionStatus":"Success","DisplayText":"..."}
    text = resp.get("DisplayText") or json.dumps(resp)
    return text

async def transcribe_file(file_path, provider="openai"):
    provider = provider.lower()
    if provider == "openai":
        # run blocking IO in thread
        return await asyncio.to_thread(transcribe_with_openai, file_path)
    elif provider == "azure":
        return await asyncio.to_thread(transcribe_with_azure, file_path)
    else:
        raise ValueError(f"Unsupported provider: {provider}")

"""
Summarizer: chunk transcripts if needed, call LLM, return structured JSON.
This example uses OpenAI Chat Completions via the HTTP endpoint.
"""

import os
import json
import math
import requests
from dotenv import load_dotenv

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

SYSTEM_PROMPT = """You are a meeting summarization assistant. Given a meeting transcript, produce a JSON object with these keys:
- short_summary: one-paragraph summary (2-3 sentences)
- decisions: list of objects {decision: str, context: str}
- action_items: list of objects {task: str, owner: str or null, due: str or null}
- participants: list of participant names (or empty)
- important_topics: list of keywords (3-8)
Return only valid JSON. If you cannot find info, return null or empty lists.
"""

def split_chunks(text, max_chars=6000):
    """
    Naive split by chars into chunks that fit model context.
    Adjust chunking logic for better semantic splits if needed.
    """
    if len(text) <= max_chars:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + max_chars)
        # try to break at newline for readability
        if end < len(text):
            newline = text.rfind("\n", start, end)
            if newline > start:
                end = newline
        chunks.append(text[start:end])
        start = end
    return chunks

def call_openai_chat(messages, max_tokens=800, temperature=0.0):
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "gpt-4o-mini",
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    r = requests.post(url, headers=headers, json=payload, timeout=120)
    r.raise_for_status()
    return r.json()

async def summarize_transcript(transcript_text, metadata=None):
    # 1) chunk transcript
    chunks = split_chunks(transcript_text, max_chars=5000)
    chunk_summaries = []

    for i, chunk in enumerate(chunks):
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Transcript chunk {i+1}/{len(chunks)}:\n\n{chunk}"}
        ]
        resp = call_openai_chat(messages)
        out = resp["choices"][0]["message"]["content"]
        # try parse JSON, fallback to raw text
        try:
            parsed = json.loads(out)
        except Exception:
            parsed = {"raw": out}
        chunk_summaries.append(parsed)

    # 2) if multiple chunks, synthesize
    if len(chunk_summaries) == 1:
        return chunk_summaries[0]
    # create a synthesis prompt: combine chunk summaries
    synth_input = json.dumps(chunk_summaries, indent=2)
    synth_messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"These are JSON summaries for transcript chunks:\n{synth_input}\n\nPlease merge them into one final JSON meeting summary with the same schema. Remove duplicates and consolidate action items."}
    ]
    resp = call_openai_chat(synth_messages, max_tokens=1000)
    out = resp["choices"][0]["message"]["content"]
    try:
        final = json.loads(out)
    except Exception:
        final = {"raw": out, "chunks": chunk_summaries}
    return final

"""
FastAPI version of Meeting Summarizer
------------------------------------
Endpoints:
  GET  /        -> health check
  POST /upload  -> upload audio -> transcribe -> summarize -> JSON result

Usage:
  1. pip install fastapi uvicorn[standard] python-multipart requests python-dotenv
  2. Set .env with OPENAI_API_KEY etc.
  3. Run: uvicorn app:app --reload --port 8000
"""

import os
import requests
from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
import aiofiles
from dotenv import load_dotenv
import json

# Load environment
load_dotenv()

# --- Config ---
UPLOAD_DIR = os.getenv("UPLOAD_DIR", "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)
PORT = int(os.getenv("PORT", 8000))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ASR_MODEL = os.getenv("OPENAI_ASR_MODEL", "whisper-1")
OPENAI_CHAT_MODEL = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")

# --- App ---
app = FastAPI(title="Meeting Summarizer API (FastAPI)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"ok": True, "msg": "Meeting Summarizer API (FastAPI)"}

@app.post("/upload")
async def upload_audio(
    file: UploadFile = File(...),
    meeting_title: str = Form(None)
):
    try:
        # 1️⃣ Save uploaded file
        filename = f"{int(os.times().elapsed)}_{file.filename}"
        file_path = os.path.join(UPLOAD_DIR, filename)
        async with aiofiles.open(file_path, "wb") as out:
            content = await file.read()
            await out.write(content)
        print(f"Received file: {file_path}")

        # 2️⃣ Transcribe
        if not OPENAI_API_KEY:
            print("Using MOCK transcript (no API key set)")
            transcript = f"MOCK TRANSCRIPT: {file.filename} (no API key)."
        else:
            transcript = transcribe_openai(file_path)

        print(f"Transcript length: {len(transcript)}")

        # 3️⃣ Summarize
        if not OPENAI_API_KEY:
            summary = {
                "short_summary": "MOCK SUMMARY — no API key provided",
                "decisions": [],
                "action_items": [],
                "participants": [],
                "important_topics": []
            }
        else:
            summary = summarize_openai(transcript)

        return {"filename": filename, "transcript": transcript, "summary": summary}
    except Exception as e:
        print("Upload error:", e)
        raise HTTPException(status_code=500, detail=str(e))

# --- Helper functions ---

def transcribe_openai(file_path: str) -> str:
    """Call OpenAI Whisper transcription API"""
    url = "https://api.openai.com/v1/audio/transcriptions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}
    with open(file_path, "rb") as f:
        files = {"file": (os.path.basename(file_path), f, "audio/wav")}
        data = {"model": OPENAI_ASR_MODEL}
        r = requests.post(url, headers=headers, files=files, data=data, timeout=300)
    r.raise_for_status()
    data = r.json()
    return data.get("text") or data.get("transcript") or json.dumps(data)

def summarize_openai(transcript: str) -> dict:
    """Call OpenAI ChatCompletion API for summarization"""
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    system_prompt = (
        "You are a meeting summarization assistant. Output JSON with fields: "
        "short_summary, decisions, action_items, participants, important_topics."
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": transcript},
    ]
    body = {
        "model": OPENAI_CHAT_MODEL,
        "messages": messages,
        "temperature": 0.0,
        "max_tokens": 800,
    }
    r = requests.post(url, headers=headers, json=body, timeout=120)
    r.raise_for_status()
    raw = r.json()["choices"][0]["message"]["content"]
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return {"raw": raw}

# --- Run locally ---
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=PORT, reload=True)